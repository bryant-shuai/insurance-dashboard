import express from 'express'
import cors from 'cors'
import multer from 'multer'
import { createRequire } from 'module'
import fs from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'

const require = createRequire(import.meta.url)
const XLSX = require('xlsx')

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

const app = express()
const PORT = process.env.PORT || 3001

app.use(cors())
app.use(express.json({ limit: '10mb' }))
app.use(express.urlencoded({ extended: true, limit: '10mb', parameterLimit: 1000000 }))

const uploadDir = path.join(__dirname, 'uploads')
const dataDir = path.join(__dirname, 'data')
const datasetsFile = path.join(dataDir, 'datasets.json')

const storage = multer.diskStorage({
  destination: (req, file, cb) => {
    cb(null, uploadDir)
  },
  filename: (req, file, cb) => {
    const timestamp = Date.now()
    const random = Math.floor(Math.random() * 10000)
    // è§£ç åŸå§‹æ–‡ä»¶åï¼Œå¤„ç†ä¸­æ–‡ä¹±ç é—®é¢˜
    const originalName = Buffer.from(file.originalname, 'latin1').toString('utf8')
    const ext = path.extname(originalName)
    cb(null, `${timestamp}-${random}${ext}`)
  }
})

const upload = multer({ 
  storage,
  fileFilter: (req, file, cb) => {
    const ext = path.extname(file.originalname).toLowerCase()
    if (['.xlsx', '.xls', '.csv'].includes(ext)) {
      cb(null, true)
    } else {
      cb(new Error('åªæ”¯æŒ .xlsx, .xls, .csv æ ¼å¼çš„æ–‡ä»¶'))
    }
  },
  limits: {
    fileSize: 10 * 1024 * 1024
  },
  preservePath: false
})

function ensureDirectories() {
  if (!fs.existsSync(uploadDir)) {
    fs.mkdirSync(uploadDir, { recursive: true })
  }
  if (!fs.existsSync(dataDir)) {
    fs.mkdirSync(dataDir, { recursive: true })
  }
  if (!fs.existsSync(datasetsFile)) {
    fs.writeFileSync(datasetsFile, JSON.stringify([], null, 2))
  }
}

function getDatasets() {
  try {
    if (fs.existsSync(datasetsFile)) {
      const data = fs.readFileSync(datasetsFile, 'utf-8')
      return JSON.parse(data)
    }
    return []
  } catch (error) {
    console.error('è¯»å–æ•°æ®é›†å¤±è´¥:', error)
    return []
  }
}

function saveDatasets(datasets) {
  try {
    const data = JSON.stringify(datasets, null, 2)
    fs.writeFileSync(datasetsFile, data, 'utf8')
    return true
  } catch (error) {
    console.error('ä¿å­˜æ•°æ®é›†å¤±è´¥:', error)
    return false
  }
}

function parseExcelFile(filePath) {
  try {
    let json = []
    let rawHtml = ''
    
    // æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    const ext = path.extname(filePath).toLowerCase()
    
    if (ext === '.csv') {
      // å¯¹äºCSVæ–‡ä»¶ï¼Œä½¿ç”¨fsè¯»å–å¹¶æŒ‡å®šUTF-8ç¼–ç 
      const content = fs.readFileSync(filePath, 'utf8')
      
      // æ‰‹åŠ¨è§£æCSV
      const lines = content.split('\n').filter(line => line.trim())
      json = lines.map(line => {
        // ç®€å•çš„CSVè§£æï¼Œå¤„ç†é€—å·åˆ†éš”
        return line.split(',').map(cell => cell.trim())
      })
      
      console.log('CSVæ–‡ä»¶è§£æå®Œæˆï¼Œè¡Œæ•°:', json.length)
      
      // ç”Ÿæˆç®€å•çš„HTMLè¡¨æ ¼
      let tableHtml = '<table border="0" cellpadding="0" cellspacing="0">'
      json.forEach(row => {
        tableHtml += '<tr>'
        row.forEach(cell => {
          tableHtml += `<td>${cell}</td>`
        })
        tableHtml += '</tr>'
      })
      tableHtml += '</table>'
      rawHtml = tableHtml
    } else {
      // å¯¹äºExcelæ–‡ä»¶ï¼Œä½¿ç”¨XLSXåº“
      const workbook = XLSX.readFile(filePath)
      const sheet = workbook.Sheets[workbook.SheetNames[0]]
      json = XLSX.utils.sheet_to_json(sheet, { header: 1 })
      
      // ç”ŸæˆHTMLè¡¨æ ¼
      rawHtml = XLSX.utils.sheet_to_html(sheet)
        .replace('<table>', '<table border="0" cellpadding="0" cellspacing="0">')
    }
    
    console.log('è§£ææ–‡ä»¶å†…å®¹ï¼Œå‰5è¡Œ:')
    for (let i = 0; i < Math.min(5, json.length); i++) {
      console.log(`ç¬¬${i}è¡Œ:`, json[i])
    }
    
    let headerIdx = -1
    let metricIdx = -1
    
    for (let i = 0; i < Math.min(20, json.length); i++) {
      const row = json[i] || []
      const str = row.join(' ')
      console.log(`æ£€æŸ¥ç¬¬${i}è¡Œ: "${str}"`)
      if (str.includes('éè½¦é™©')) {
        headerIdx = i
        console.log(`æ‰¾åˆ°è¡¨å¤´è¡Œ: ${i}`)
      }
      // åªè¦è¡Œä¸­åŒ…å«"æœ¬æœŸç´¯è®¡"æˆ–"åŒæ¯”å¢é•¿"ï¼Œå°±è®¤ä¸ºæ˜¯æŒ‡æ ‡è¡Œ
      if (str.includes('æœ¬æœŸç´¯è®¡') || str.includes('åŒæ¯”å¢é•¿')) {
        metricIdx = i
        console.log(`æ‰¾åˆ°æŒ‡æ ‡è¡Œ: ${i}`)
      }
    }

    console.log(`è¡¨å¤´è¡Œç´¢å¼•: ${headerIdx}, æŒ‡æ ‡è¡Œç´¢å¼•: ${metricIdx}`)

    if (headerIdx === -1 || metricIdx === -1) {
      throw new Error('è¡¨å¤´è¯†åˆ«å¤±è´¥')
    }

    const headers = json[headerIdx]
    const metrics = json[metricIdx]
    const colMap = {}
    let tempIns = ''
    const maxCol = Math.max(headers.length, metrics.length)
    const insurances = []
    const companies = {}

    // é‡æ–°æ„å»ºåˆ—æ˜ å°„ï¼Œå¤„ç†CSVæ ¼å¼
    // CSVæ ¼å¼ï¼šåœ°åŒº,å…¬å¸åç§°,é™©ç§1,é™©ç§2,...
    // æŒ‡æ ‡è¡Œï¼šæœ¬æœŸç´¯è®¡,åŒæ¯”å¢é•¿,æœ¬æœŸç´¯è®¡,åŒæ¯”å¢é•¿,...
    // æ¯ä¸ªé™©ç§å¯¹åº”ä¸¤åˆ—ï¼šæœ¬æœŸç´¯è®¡å’ŒåŒæ¯”å¢é•¿
    
    // ä»è¡¨å¤´è¡Œæå–é™©ç§åˆ—è¡¨ï¼ˆä»ç¬¬2åˆ—å¼€å§‹ï¼‰
    for (let i = 2; i < headers.length; i++) {
      const header = headers[i]
      
      if (header && !['åœ°åŒº', 'å…¬å¸åç§°'].includes(header)) {
        const insName = header.trim()
        
        if (!insurances.includes(insName)) {
          insurances.push(insName)
          colMap[insName] = {}
        }
      }
    }

    // ä»æŒ‡æ ‡è¡Œæå–æ¯ä¸ªé™©ç§çš„åˆ—ç´¢å¼•
    // æŒ‡æ ‡è¡Œçš„ç»“æ„ï¼šæœ¬æœŸç´¯è®¡,åŒæ¯”å¢é•¿,æœ¬æœŸç´¯è®¡,åŒæ¯”å¢é•¿,...
    // æ¯ä¸ªé™©ç§å¯¹åº”ä¸¤åˆ—ï¼šæœ¬æœŸç´¯è®¡å’ŒåŒæ¯”å¢é•¿
    // ä½¿ç”¨é™©ç§ç´¢å¼•æ¥è·Ÿè¸ªå½“å‰å¤„ç†çš„é™©ç§
    let insIndex = 0
    
    // éå†æŒ‡æ ‡è¡Œï¼Œä»ç¬¬2åˆ—å¼€å§‹ï¼ˆè·³è¿‡åœ°åŒºå’Œå…¬å¸åç§°åˆ—ï¼‰
    for (let i = 2; i < metrics.length; i++) {
      const metric = metrics[i]
      
      if (metric && insIndex < insurances.length) {
        const insName = insurances[insIndex]
        const metricStr = metric.toString().trim()
        
        if (metricStr.includes('æœ¬æœŸç´¯è®¡')) {
          colMap[insName].p = i
        } else if (metricStr.includes('åŒæ¯”å¢é•¿')) {
          colMap[insName].g = i
          insIndex++ // åªæœ‰å¤„ç†å®ŒåŒæ¯”å¢é•¿åæ‰ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªé™©ç§
        }
      }
    }

    console.log('åˆ—æ˜ å°„:', colMap)
    console.log('é™©ç§åˆ—è¡¨:', insurances)

    for (let i = metricIdx + 1; i < json.length; i++) {
      const row = json[i]
      if (!row || row.length < 2) continue
      const region = row[0] || ''
      const name = row[1] || ''
      if (region.includes('åˆè®¡') || name.includes('åˆè®¡') || !name) continue

      const fullName = region ? `${region}-${name}` : name
      companies[fullName] = {}

      insurances.forEach(ins => {
        const map = colMap[ins]
        if (map && map.p !== undefined) {
          let p = parseFloat(row[map.p]) || 0
          let gStr = map.g !== undefined ? String(row[map.g]).replace('%', '') : '0'
          let g = parseFloat(gStr)
          if (isNaN(g)) g = 0
          companies[fullName][ins] = { premium: p, growth: g }
        }
      })
    }

    insurances.sort((a, b) => a === 'éè½¦é™©' ? -1 : 1)

    return { insurances, companies, rawHtml }
  } catch (error) {
    throw error
  }
}

app.get('/api/datasets', (req, res) => {
  try {
    const datasets = getDatasets()
    res.json(datasets)
  } catch (error) {
    res.status(500).json({ error: 'è·å–æ•°æ®é›†å¤±è´¥' })
  }
})

app.get('/api/datasets/:id', (req, res) => {
  try {
    const datasets = getDatasets()
    const dataset = datasets.find(d => d.id === req.params.id)
    if (!dataset) {
      return res.status(404).json({ error: 'æ•°æ®é›†ä¸å­˜åœ¨' })
    }
    res.json(dataset)
  } catch (error) {
    res.status(500).json({ error: 'è·å–æ•°æ®é›†å¤±è´¥' })
  }
})

// è·å–åˆ†ææ•°æ®ï¼ˆä¸ç”Ÿäº§ä¸€è‡´ï¼‰
app.get('/api/analysis/:id', (req, res) => {
  try {
    const datasets = getDatasets()
    const dataset = datasets.find(d => d.id === req.params.id)
    
    if (!dataset) {
      return res.status(404).json({ error: 'æ•°æ®é›†ä¸å­˜åœ¨' })
    }

    const companies = dataset.companies || {}
    const primaryInsurance = 'éè½¦é™©'

    // æå–éè½¦é™©æ•°æ®
    const nonAutoData = {}
    Object.entries(companies).forEach(([key, value]) => {
      if (value[primaryInsurance]) {
        nonAutoData[key] = value[primaryInsurance]
      }
    })

    // è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    let totalPremium = 0
    let totalGrowth = 0
    let count = 0
    
    Object.values(nonAutoData).forEach(item => {
      totalPremium += item.premium || 0
      totalGrowth += item.growth || 0
      count++
    })
    
    const avgGrowth = count > 0 ? (totalGrowth / count).toFixed(2) : 0
    const avgShare = 100 / count

    // æ„é€ BCGçŸ©é˜µæ•°æ® â€” ä½¿ç”¨å¹³å‡å¢é€Ÿå’Œå¹³å‡ä»½é¢ä½œä¸ºé˜ˆå€¼
    const bcgMatrix = Object.entries(nonAutoData).map(([key, value]) => {
      const premium = value.premium || 0
      const growth = value.growth || 0
      const share = totalPremium > 0 ? (premium / totalPremium) * 100 : 0

      let quadrant = 'ç˜¦ç‹—'
      if (growth >= avgGrowth && share >= avgShare) {
        quadrant = 'æ˜æ˜Ÿ'
      } else if (growth < avgGrowth && share >= avgShare) {
        quadrant = 'å¥¶ç‰›'
      } else if (growth >= avgGrowth && share < avgShare) {
        quadrant = 'é‡çŒ«'
      }
      
      return {
        name: key.split('-').pop() || key,
        x: parseFloat(share.toFixed(2)),
        y: parseFloat(growth.toFixed(2)),
        z: premium,
        quadrant,
        premium,
        growth,
        share: parseFloat(share.toFixed(2))
      }
    })

    // CR4
    const topCompanies = [...bcgMatrix]
      .sort((a, b) => b.z - a.z)
      .slice(0, 4)
    const cr4 = Math.round(topCompanies.reduce((sum, item) => sum + item.share, 0))

    const analysisData = {
      id: dataset.id,
      name: dataset.name,
      market_insight: {
        'éè½¦é™©': {
          total_premium: totalPremium,
          avg_growth: parseFloat(avgGrowth),
          market_type: cr4 > 60 ? 'é«˜åº¦é›†ä¸­' : cr4 > 30 ? 'ä¸­åº¦é›†ä¸­' : 'åˆ†æ•£ç«äº‰',
          cr4: cr4,
          top_companies: topCompanies.map(item => ({
            company: item.name,
            premium: item.z,
            growth: item.y,
            share: item.share
          }))
        }
      },
      bcg_matrix: bcgMatrix,
      summary: {
        total_premium: totalPremium,
        avg_growth: parseFloat(avgGrowth),
        company_count: count,
        market_type: cr4 > 60 ? 'é«˜åº¦é›†ä¸­' : cr4 > 30 ? 'ä¸­åº¦é›†ä¸­' : 'åˆ†æ•£ç«äº‰',
        cr4: cr4
      },
      createdAt: dataset.createdAt
    }

    res.json(analysisData)
  } catch (error) {
    console.error('è·å–åˆ†ææ•°æ®å¤±è´¥:', error)
    res.status(500).json({ error: 'è·å–åˆ†ææ•°æ®å¤±è´¥' })
  }
})

app.post('/api/upload', upload.single('file'), (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶' })
    }

    // è§£ç åŸå§‹æ–‡ä»¶åï¼Œå¤„ç†ä¸­æ–‡ä¹±ç é—®é¢˜
    const decodedOriginalName = Buffer.from(req.file.originalname, 'latin1').toString('utf8')
    console.log('ä¸Šä¼ çš„æ–‡ä»¶å:', decodedOriginalName)
    console.log('ä¿å­˜çš„æ–‡ä»¶å:', req.file.filename)

    let insurances = []
    let companies = {}
    let rawHtml = ''

    try {
      const result = parseExcelFile(req.file.path)
      insurances = result.insurances
      companies = result.companies
      rawHtml = result.rawHtml
    } catch (error) {
      console.error('è§£ææ–‡ä»¶å¤±è´¥:', error)
    }

    const fileName = decodedOriginalName.replace(/\.[^/.]+$/, '')
    const datasetId = Date.now().toString()

    const newDataset = {
      id: datasetId,
      name: fileName,
      fileName: req.file.filename,
      insurances,
      companies,
      rawHtml,
      createdAt: new Date().toISOString()
    }

    const datasets = getDatasets()
    datasets.push(newDataset)
    saveDatasets(datasets)

    res.json(newDataset)
  } catch (error) {
    console.error('ä¸Šä¼ æ–‡ä»¶å¤±è´¥:', error)
    res.status(500).json({ error: error.message || 'ä¸Šä¼ æ–‡ä»¶å¤±è´¥' })
  }
})

app.delete('/api/datasets/:id', (req, res) => {
  try {
    const datasets = getDatasets()
    const index = datasets.findIndex(d => d.id === req.params.id)
    
    if (index === -1) {
      return res.status(404).json({ error: 'æ•°æ®é›†ä¸å­˜åœ¨' })
    }

    const dataset = datasets[index]
    if (dataset.fileName) {
      const filePath = path.join(uploadDir, dataset.fileName)
      if (fs.existsSync(filePath)) {
        fs.unlinkSync(filePath)
      }
    }

    datasets.splice(index, 1)
    saveDatasets(datasets)

    res.json({ success: true })
  } catch (error) {
    console.error('åˆ é™¤æ•°æ®é›†å¤±è´¥:', error)
    res.status(500).json({ error: 'åˆ é™¤æ•°æ®é›†å¤±è´¥' })
  }
})

app.put('/api/datasets/:id', (req, res) => {
  try {
    const { name } = req.body
    if (!name) {
      return res.status(400).json({ error: 'åç§°ä¸èƒ½ä¸ºç©º' })
    }

    const datasets = getDatasets()
    const dataset = datasets.find(d => d.id === req.params.id)
    
    if (!dataset) {
      return res.status(404).json({ error: 'æ•°æ®é›†ä¸å­˜åœ¨' })
    }

    dataset.name = name
    saveDatasets(datasets)

    res.json(dataset)
  } catch (error) {
    console.error('æ›´æ–°æ•°æ®é›†å¤±è´¥:', error)
    res.status(500).json({ error: 'æ›´æ–°æ•°æ®é›†å¤±è´¥' })
  }
})

app.listen(PORT, () => {
  ensureDirectories()
  console.log(`ğŸš€ æœåŠ¡å™¨è¿è¡Œåœ¨ http://localhost:${PORT}`)
  console.log(`ğŸ“ ä¸Šä¼ ç›®å½•: ${uploadDir}`)
  console.log(`ğŸ“Š æ•°æ®ç›®å½•: ${dataDir}`)
})
