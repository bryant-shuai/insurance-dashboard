import os
import json
import uuid
from datetime import datetime
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from werkzeug.utils import secure_filename
from excel_parser import ExcelParser
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# é…ç½®
UPLOAD_FOLDER = os.path.join(os.path.dirname(__file__), 'uploads')
DATA_FOLDER = os.path.join(os.path.dirname(__file__), 'data')
DATASETS_FILE = os.path.join(DATA_FOLDER, 'datasets.json')
ALLOWED_EXTENSIONS = {'.xlsx', '.xls', '.csv'}

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 10 * 1024 * 1024  # 10MB

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(DATA_FOLDER, exist_ok=True)

if not os.path.exists(DATASETS_FILE):
    with open(DATASETS_FILE, 'w', encoding='utf-8') as f:
        json.dump([], f, ensure_ascii=False, indent=2)

def allowed_file(filename):
    return '.' in filename and os.path.splitext(filename)[1].lower() in ALLOWED_EXTENSIONS

def get_datasets():
    """è·å–æ‰€æœ‰æ•°æ®é›†"""
    try:
        with open(DATASETS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"è¯»å–æ•°æ®é›†å¤±è´¥: {str(e)}")
        return []

def save_datasets(datasets):
    """ä¿å­˜æ•°æ®é›†"""
    try:
        with open(DATASETS_FILE, 'w', encoding='utf-8') as f:
            json.dump(datasets, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜æ•°æ®é›†å¤±è´¥: {str(e)}")
        return False

@app.route('/api/datasets', methods=['GET'])
def get_all_datasets():
    """è·å–æ‰€æœ‰æ•°æ®é›†åˆ—è¡¨"""
    try:
        datasets = get_datasets()
        return jsonify(datasets)
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({'error': 'è·å–æ•°æ®é›†å¤±è´¥'}), 500

@app.route('/api/datasets/<dataset_id>', methods=['GET'])
def get_dataset(dataset_id):
    """è·å–ç‰¹å®šæ•°æ®é›†"""
    try:
        datasets = get_datasets()
        dataset = next((d for d in datasets if d['id'] == dataset_id), None)
        
        if not dataset:
            return jsonify({'error': 'æ•°æ®é›†ä¸å­˜åœ¨'}), 404
            
        return jsonify(dataset)
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†å¤±è´¥: {str(e)}")
        return jsonify({'error': 'è·å–æ•°æ®é›†å¤±è´¥'}), 500

@app.route('/api/analysis/<dataset_id>', methods=['GET'])
def get_analysis_data(dataset_id):
    """è·å–åˆ†ææ•°æ®"""
    try:
        datasets = get_datasets()
        dataset = next((d for d in datasets if d['id'] == dataset_id), None)
        
        if not dataset:
            return jsonify({'error': 'æ•°æ®é›†ä¸å­˜åœ¨'}), 404

        companies = dataset.get('companies', {})
        primary_insurance = 'éè½¦é™©'

        # æå–éè½¦é™©æ•°æ®
        non_auto_data = {}
        for key, value in companies.items():
            if primary_insurance in value:
                non_auto_data[key] = value[primary_insurance]

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        total_premium = 0
        total_growth = 0
        count = 0
        
        for item in non_auto_data.values():
            total_premium += item.get('premium', 0)
            total_growth += item.get('growth', 0)
            count += 1
        
        avg_growth = round(total_growth / count, 2) if count > 0 else 0
        avg_share = 100 / count if count > 0 else 0

        # æ„é€ BCGçŸ©é˜µæ•°æ®
        bcg_matrix = []
        for key, value in non_auto_data.items():
            premium = value.get('premium', 0)
            growth = value.get('growth', 0)
            share = (premium / total_premium) * 100 if total_premium > 0 else 0

            # ç¡®å®šè±¡é™
            if growth >= avg_growth and share >= avg_share:
                quadrant = 'æ˜æ˜Ÿ'
            elif growth < avg_growth and share >= avg_share:
                quadrant = 'å¥¶ç‰›'
            elif growth >= avg_growth and share < avg_share:
                quadrant = 'é‡çŒ«'
            else:
                quadrant = 'ç˜¦ç‹—'

            bcg_matrix.append({
                'name': key.split('-')[-1] if '-' in key else key,
                'x': round(share, 2),
                'y': round(growth, 2),
                'z': premium,
                'quadrant': quadrant,
                'premium': premium,
                'growth': growth,
                'share': round(share, 2)
            })

        # CR4è®¡ç®—
        top_companies = sorted(bcg_matrix, key=lambda x: x['z'], reverse=True)[:4]
        cr4 = round(sum(item['share'] for item in top_companies))

        analysis_data = {
            'id': dataset['id'],
            'name': dataset['name'],
            'market_insight': {
                'éè½¦é™©': {
                    'total_premium': total_premium,
                    'avg_growth': avg_growth,
                    'market_type': 'é«˜åº¦é›†ä¸­' if cr4 > 60 else ('ä¸­åº¦é›†ä¸­' if cr4 > 30 else 'åˆ†æ•£ç«äº‰'),
                    'cr4': cr4,
                    'top_companies': [{
                        'company': item['name'],
                        'premium': item['z'],
                        'growth': item['y'],
                        'share': item['share']
                    } for item in top_companies]
                }
            },
            'bcg_matrix': bcg_matrix,
            'summary': {
                'total_premium': total_premium,
                'avg_growth': avg_growth,
                'company_count': count,
                'market_type': 'é«˜åº¦é›†ä¸­' if cr4 > 60 else ('ä¸­åº¦é›†ä¸­' if cr4 > 30 else 'åˆ†æ•£ç«äº‰'),
                'cr4': cr4
            },
            'createdAt': dataset['createdAt']
        }

        return jsonify(analysis_data)
    except Exception as e:
        logger.error(f"è·å–åˆ†ææ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({'error': 'è·å–åˆ†ææ•°æ®å¤±è´¥'}), 500

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """ä¸Šä¼ æ–‡ä»¶ - å¢å¼ºç‰ˆ"""
    try:
        logger.info("æ”¶åˆ°æ–‡ä»¶ä¸Šä¼ è¯·æ±‚")
        
        # æ£€æŸ¥æ–‡ä»¶
        if 'file' not in request.files:
            logger.error("è¯·æ±‚ä¸­æ²¡æœ‰æ–‡ä»¶")
            return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400
        
        file = request.files['file']
        if file.filename == '':
            logger.error("æ–‡ä»¶åä¸ºç©º")
            return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not allowed_file(file.filename):
            logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.filename}")
            return jsonify({'error': 'åªæ”¯æŒ .xlsx, .xls, .csv æ ¼å¼çš„æ–‡ä»¶'}), 400
        
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}")
        
        # ä¿å­˜æ–‡ä»¶
        original_filename = file.filename
        timestamp = int(datetime.now().timestamp() * 1000)
        random_num = str(uuid.uuid4().int)[:4]
        ext = os.path.splitext(original_filename)[1]
        new_filename = f"{timestamp}-{random_num}{ext}"
        
        file_path = os.path.join(UPLOAD_FOLDER, new_filename)
        file.save(file_path)
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
        
        # è§£ææ–‡ä»¶
        parser = ExcelParser()
        try:
            result = parser.parse_file(file_path)
            logger.info("æ–‡ä»¶è§£ææˆåŠŸ")
        except Exception as parse_error:
            logger.error(f"æ–‡ä»¶è§£æå¤±è´¥: {str(parse_error)}")
            # åˆ é™¤å·²ä¿å­˜çš„æ–‡ä»¶
            if os.path.exists(file_path):
                os.remove(file_path)
            return jsonify({'error': f'æ–‡ä»¶è§£æå¤±è´¥: {str(parse_error)}'}), 400
        
        # åˆ›å»ºæ•°æ®é›†è®°å½•
        dataset_id = str(int(datetime.now().timestamp() * 1000))
        # ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºæ•°æ®é›†åç§°
        file_name_without_ext = os.path.splitext(original_filename)[0]
        
        new_dataset = {
            'id': dataset_id,
            'name': file_name_without_ext,
            'fileName': new_filename,
            'insurances': result['insurances'],
            'companies': result['companies'],
            'rawHtml': result['raw_html'],
            'createdAt': datetime.now().isoformat()
        }
        
        logger.info(f"åˆ›å»ºæ•°æ®é›†è®°å½•ï¼ŒID: {dataset_id}")
        
        # ä¿å­˜åˆ°æ•°æ®é›†
        datasets = get_datasets()
        datasets.append(new_dataset)
        if save_datasets(datasets):
            logger.info("æ•°æ®é›†ä¿å­˜æˆåŠŸ")
        else:
            logger.error("æ•°æ®é›†ä¿å­˜å¤±è´¥")
            return jsonify({'error': 'æ•°æ®ä¿å­˜å¤±è´¥'}), 500
        
        logger.info(f"æ–‡ä»¶ä¸Šä¼ å®Œæˆ: {file.filename}")
        return jsonify(new_dataset)
        
    except Exception as e:
        logger.error(f"ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        return jsonify({'error': f'ä¸Šä¼ å¤±è´¥: {str(e)}'}), 500

@app.route('/api/datasets/<dataset_id>', methods=['DELETE'])
def delete_dataset(dataset_id):
    """åˆ é™¤æ•°æ®é›†"""
    try:
        datasets = get_datasets()
        dataset = next((d for d in datasets if d['id'] == dataset_id), None)
        
        if not dataset:
            return jsonify({'error': 'æ•°æ®é›†ä¸å­˜åœ¨'}), 404

        # åˆ é™¤æ–‡ä»¶
        if 'fileName' in dataset:
            file_path = os.path.join(UPLOAD_FOLDER, dataset['fileName'])
            if os.path.exists(file_path):
                os.remove(file_path)

        # ä»æ•°æ®é›†ä¸­åˆ é™¤
        datasets = [d for d in datasets if d['id'] != dataset_id]
        save_datasets(datasets)

        return jsonify({'success': True})
    except Exception as e:
        logger.error(f"åˆ é™¤æ•°æ®é›†å¤±è´¥: {str(e)}")
        return jsonify({'error': 'åˆ é™¤æ•°æ®é›†å¤±è´¥'}), 500

@app.route('/api/datasets/<dataset_id>', methods=['PUT'])
def update_dataset(dataset_id):
    """æ›´æ–°æ•°æ®é›†"""
    try:
        data = request.get_json()
        name = data.get('name')
        
        if not name:
            return jsonify({'error': 'åç§°ä¸èƒ½ä¸ºç©º'}), 400

        datasets = get_datasets()
        dataset = next((d for d in datasets if d['id'] == dataset_id), None)
        
        if not dataset:
            return jsonify({'error': 'æ•°æ®é›†ä¸å­˜åœ¨'}), 404

        dataset['name'] = name
        save_datasets(datasets)

        return jsonify(dataset)
    except Exception as e:
        logger.error(f"æ›´æ–°æ•°æ®é›†å¤±è´¥: {str(e)}")
        return jsonify({'error': 'æ›´æ–°æ•°æ®é›†å¤±è´¥'}), 500

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 3001))
    print(f"ğŸš€ æœåŠ¡å™¨è¿è¡Œåœ¨ http://localhost:{port}")
    print(f"ğŸ“ ä¸Šä¼ ç›®å½•: {UPLOAD_FOLDER}")
    print(f"ğŸ“Š æ•°æ®ç›®å½•: {DATA_FOLDER}")
    app.run(host='0.0.0.0', port=port, debug=True)